<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sucheng Ren</title>

  <meta name="author" content="Sucheng Ren">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table
    style="width:110%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Sucheng (Oliver) Ren</name>
                  </p>
                  <p>
                    Hi, I am Sucheng Ren (‰ªªËãèÊàê), a Computer Science Ph.D. student at Johns Hopkins University, where I am fortunate to be advised by Bloomberg Distinguished Professor <a href="http://www.cs.jhu.edu/~ayuille/"> Alan Yuille</a> and Prof. <a href="https://cihangxie.github.io/"> Cihang Xie</a>. I received my B.S. and M.S. degree in Computer Science from South China University of Technology. Previously, I spent great time at Microsoft Research Asia (MSRA), Tsinghua University and National University of Singapore.
                  </p>
                  <p>
                    My research lies at the Self-Supervised Learning and Multimodal Learning.
                  </p>
                  <p style="color:red;">
                    I'm currently looking for research intern roles for Summer 2024!
                  </p>

                  <p style="text-align:center">
                    <a href="mailto:oliverrensu@gmail.com">Email</a> &nbsp|&nbsp
                    <a href="data/Sucheng_Ren_cv.pdf">CV</a> &nbsp|&nbsp
                    <a href="https://scholar.google.com/citations?user=Hbf-SoAAAAAJ&hl=zh-CN">Scholar</a> &nbsp|&nbsp
                    <a href="https://github.com/OliverRensu">Github</a> &nbsp|&nbsp
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/profile.jpg"><img style="width:110%;max-width:110%" alt="profile photo"
                      src="images/profile.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>



          <table width="110%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>News</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <ul>
                <li>[Aug. 2023] Join Johns Hopkins University as a PhD student!</li>
                <li>[Jul. 2023] SG-Former got accepted by <strong>ICCV2022</strong>!üéâ</li>
                <li>[Feb. 2023] TinyMIM got accepted by <strong>CVPR2023</strong>!üéâ</li>
                <li>[Jun. 2022] Working with Dr. <a href="https://ancientmooner.github.io/">Han Hu</a> and <a href="https://scholar.google.com/citations?user=-ncz2s8AAAAJ&hl=en"> Fangyun Wei</a> at Microsoft Research Asia (MSRA)!üë©‚Äçüíª</li>
                <li>[Feb. 2022] <strong>Three</strong> first author papers got accepted by <strong>CVPR2022</strong> including one <strong>Oral</strong> paper!üéâ</li>
                <li>[Dec. 2021] Working with Prof. <a href="https://scholar.google.com.hk/citations?user=Q8iay0gAAAAJ&hl=zh-CN">Jiashi Feng</a> at National University of Singapore!üë©‚Äçüíª</li>
                <li>[Jun. 2021] Working with Prof. <a href="https://cihangxie.github.io/">Cihang Xie</a> and Prof. <a href="http://www.cs.jhu.edu/~ayuille/"> Alan Yuille</a> at John Hopkins University!üë©‚Äçüíª</li>
                <li>[Jul. 2021] <strong>Two</strong> papers got accepted by <strong>ICCV2021</strong> !üéâ</li>
                <li>[May. 2021] Working with Prof. <a href="https://cihangxie.github.io/">Cihang Xie</a> and Prof. <a href="http://www.cs.jhu.edu/~ayuille/"> Alan Yuille</a> at John Hopkins University!üë©‚Äçüíª</li>
                <li>[Mar. 2021] <strong>Three</strong> papers got accepted by <strong>CVPR2021</strong> including two first author papers!üéâ</li>
                <li>[Dec. 2020] Working with Prof. <a href="https://hangzhaomit.github.io/">Hang Zhao</a> in Tsinghua as a research intern!üë©‚Äçüíª</li>
                </ul>
            </tbody>
          </table>



          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:110%;vertical-align:middle">
                  <heading>Selected Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table
          style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
              <td style="padding:20px;width:35%;vertical-align:left">
                <div><img style="width:100%;max-width:100%" src="images/TinyMIM.png"></div>
              </td>
              <td style="padding:30px;width:60%;vertical-align:middle">
                  <papertitle>TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models     
                  </papertitle>
                <br>
                <br>
                <strong>Sucheng Ren</strong>,
                <a href="https://scholar.google.com/citations?user=-ncz2s8AAAAJ&hl=en">Fangyun Wei</a>,
                <a href="https://stupidzz.github.io/">Zheng Zhang</a>,
                <a href="https://ancientmooner.github.io/">Han Hu</a>

                <br>
                IEEE Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2023
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ren_TinyMIM_An_Empirical_Study_of_Distilling_MIM_Pre-Trained_Models_CVPR_2023_paper.html">[paper]</a>
                <a href="https://github.com/OliverRensu/TinyMIM">[code]</a>
                <a href="data/TinyMIM.bib">[bibtex]</a>
                <p>
                  We explore distillation techniques to transfer the success of large MIM-based pre-trained models to smaller ones.
                </p>
              </td>
            </tr>
        </table>


          <table
          style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
              <td style="padding:20px;width:35%;vertical-align:left">
                <div><img style="width:100%;max-width:100%" src="images/Shunted.png"></div>
              </td>
              <td style="padding:30px;width:60%;vertical-align:middle">
                  <papertitle>Shunted Self-Attention via Multi-Scale Token Aggregation     
                  </papertitle>
                <br>
                <br>
                <strong>Sucheng Ren</strong>, 
                <a href="https://zhengqigao.github.io/">Daquan Zhou</a>,
                <a href="http://www.shengfenghe.com/">Shengfeng He</a>,
                <a href="https://sites.google.com/site/jshfeng/home">Jiashi Feng</a>,
                <a href="https://sites.google.com/site/sitexinchaowang/">Xinchao Wang</a>

                <br>
                IEEE Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, <strong>(Oral)</strong>, 2022
                <br>
                <a href="https://arxiv.org/abs/2111.15193">[paper]</a>
                <a href="https://github.com/oliverrensu/shunted-transformer">[code]</a>
                <a href="data/Shunted.bib">[bibtex]</a>
                <p>
                  Integrating the capability of capturing multiscale objects in each attention layer by adaptively merging tokens.
                </p>
              </td>
            </tr>
        </table>


        <table
          style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
              <td style="padding:20px;width:35%;vertical-align:left">
                <div><img style="width:100%;max-width:100%" src="images/SGFormer.png"></div>
              </td>
              <td style="padding:30px;width:60%;vertical-align:middle">
                  <papertitle>SG-Former: Self-guided Transformer with Evolving Token Reallocation  
                  </papertitle>
                <br>
                <br>
                <strong>Sucheng Ren</strong>, 
                <a href="https://adamdad.github.io/"> Xingyi Yang</a>,
                <a href="https://scholar.google.com/citations?user=AnYh2rAAAAAJ&hl=en">Songhua Liu</a>,
                <a href="https://sites.google.com/site/sitexinchaowang/">Xinchao Wang</a>

                <br>
                International Conference on Computer Vision <strong>(ICCV)</strong>, 2023
                <br>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ren_SG-Former_Self-guided_Transformer_with_Evolving_Token_Reallocation_ICCV_2023_paper.pdf">[paper]</a>
                <a href="https://github.com/OliverRensu/SG-Former">[code]</a>
                <a href="data/SGFormer.bib">[bibtex]</a>
                <p>
                  Integrating the capability of capturing multiscale objects in each attention layer by adaptively merging tokens.
                </p>
              </td>
            </tr>
        </table>  

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:35%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/coadvise.png"></div>
                </td>
                <td style="padding:30px;width:60%;vertical-align:middle">
                    <papertitle>Co-advise: Cross Inductive Bias Distillation         
                    </papertitle>
                  <br>
                  <br>
                  <strong>Sucheng Ren</strong>, 
                  <a href="https://zhengqigao.github.io/">Zhengqi Gao</a>,
                  <a href="https://patrickhua.github.io/">Tianyu Hua</a>,
                  <a href="https://zihuixue.github.io/">Zihui Xue</a>,
                  <a href="https://people.csail.mit.edu/yonglong/">Yonglong Tian</a>,
                  <a href="http://www.shengfenghe.com/">Shengfeng He</a>,
                  <a href="https://hangzhaomit.github.io/">Hang Zhao</a>

                  <br>
                  IEEE Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2106.12378">[paper]</a>
                  <a href="https://github.com/OliverRensu/co-advise/">[code]</a>
                  <a href="data/coadvise.bib">[bibtex]</a>
                  <p>
                    The first work delves into the influence of models inductive biases in knowledge distillation
                  </p>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:35%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/SDMP.png"></div>
                </td>
                <td style="padding:30px;width:60%;vertical-align:middle">
                    <papertitle>A Simple Data Mixing Prior for Improving Self-Supervised Learning                    
                    </papertitle>
                  <br>
                  <br>
                  <strong>Sucheng Ren</strong>, 
                  <a href="https://zhengqigao.github.io/">Huiyu Wang</a>, 
                  <a href="https://zhengqigao.github.io/">Zhengqi Gao</a>,
                  <a href="http://www.shengfenghe.com/">Shengfeng He</a>,
                  <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                  <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                  <a href="https://cihangxie.github.io/">Cihang Xie</a>

                  <br>
                  IEEE Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2022
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ren_A_Simple_Data_Mixing_Prior_for_Improving_Self-Supervised_Learning_CVPR_2022_paper.pdf">[paper]</a>
                  <a href="data/SDMP.bib">[bibtex]</a>
                  <p>
                    A generic training strategy in data mixing that can improve the self-supervised representation learning of both CNNs and ViTs
                  </p>
                </td>
              </tr>
          </table>



          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:35%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/MKE.png"></div>
                </td>
                <td style="padding:30px;width:60%;vertical-align:middle">
                    <papertitle>Multimodal Knowledge Expansion                      
                    </papertitle>
                  <br>
                  <br>
                  <a href="https://zihuixue.github.io/">Zihui Xue</a>,
                  <strong>Sucheng Ren</strong>, 
                  <a href="https://zhengqigao.github.io/">Zhengqi Gao</a>, 
                  <a href="https://hangzhaomit.github.io/">Hang Zhao</a>

                  <br>
                  International Conference on Computer Vision <strong>(ICCV)</strong>, 2021
                  <br>
                  <a href="https://arxiv.org/abs/2103.14431">[paper]</a>
                  <a href="https://tsinghua-mars-lab.github.io/MKE/">[website]</a>
                  <a href="data/MKE.bib">[bibtex]</a>
                  <p>
                    A knowledge distillation-based framework to effectively utilize multimodal data without requiring labels.
                  </p>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:35%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/lipreading.png"></div>
                </td>
                <td style="padding:30px;width:60%;vertical-align:middle">
                    <papertitle>Learning from the Master: Distilling Cross-modal Advanced Knowledge for Lip Reading                      
                    </papertitle>
                  <br>
                  <strong>Sucheng Ren, </strong>
                  Yong Du,
                  Jianming Lv,
                  Guoqiang Han, 
                  and <a href="http://www.shengfenghe.com">Shengfeng He</a>

                  <br>
                  IEEE Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2021
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Learning_From_the_Master_Distilling_Cross-Modal_Advanced_Knowledge_for_Lip_CVPR_2021_paper.pdf">[paper]</a>
                  <!--<a href="https://arxiv.org/pdf/2012.01050.pdf">[paper]</a>-->
                  <a href="data/lipreading.bib">[bibtex]</a>
                  <p>
                    Training a master to learn how to teach a better student.
                  </p>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:35%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/RTM.png"></div>
                </td>
                <td style="padding:30px;width:60%;vertical-align:middle">
                    <papertitle>Reciprocal Transformations for Unsupervised Video Object Segmentation
                    </papertitle>
                  </a>
                  <br>
                  <br>
                  <strong>Sucheng Ren, </strong>
                  Wenxi Liu,
                  Yongtuo Liu,
                  Haoxin Chen,
                  Guoqiang Han and
                  <a href="http://www.shengfenghe.com">Shengfeng He</a>

                  <br>
                  IEEE Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2021
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Reciprocal_Transformations_for_Unsupervised_Video_Object_Segmentation_CVPR_2021_paper.pdf">[paper]</a>
                  <!--<a href="https://arxiv.org/pdf/2012.01050.pdf">[paper]</a>-->
                  <a href="data/RTM.bib">[bibtex]</a>
                  <a href="https://github.com/OliverRensu/RTNet">[code]</a>
                  <p>
                    Jointly learning salient objects, moving objects, recurring objects for Unsupervised Video Object Segmentation.
                  </p>
                </td>
              </tr>
          </table>
          
          
          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:35%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/TENET.png"></div>
                </td>
                <td style="padding:30px;width:60%;vertical-align:middle">
                    <papertitle>TENet: Triple Excitation Network for Video Salient Object Detection
                    </papertitle>
                  </a>
                  <br>
                  <br>
                  <strong>Sucheng Ren,</strong>
                  Chu Han,
                  Xin Yang,
                  Guoqiang Han and
                  <a href="http://www.shengfenghe.com">Shengfeng He</a>

                  <br>
                  European Conference on Computer Vision <strong>(ECCV)</strong>, 2020
                  <br>
                  (<strong>Spotlight</strong>, Acceptance Rate 5.0%)
                  <br>
                  <a href="https://arxiv.org/abs/2007.09943">[paper]</a>
                  <a href="data/TENet.bib">[bibtex]</a>
                  <p>
                    
                  </p>
                </td>
              </tr>
          </table>

          



  <!-- Default Statcounter code for Personal Website
https://oliverrensu.github.io/ -->
<script type="text/javascript">
  var sc_project=12694920; 
  var sc_invisible=1; 
  var sc_security="f1698122"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="Web Analytics Made Easy -
  Statcounter" href="https://statcounter.com/" target="_blank"><img
  class="statcounter" src="https://c.statcounter.com/12694920/0/f1698122/1/"
  alt="Web Analytics Made Easy - Statcounter"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->
</body>
</html>